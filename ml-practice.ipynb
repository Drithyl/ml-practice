{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659bfd5b",
   "metadata": {},
   "source": [
    "## 0. Required Modules\n",
    "\n",
    "The required modules imported below are listed in the requirements.txt file in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2349ff3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "from sklearn import datasets \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccfd29",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis \n",
    "\n",
    "In this coursework we are going to be working with the **Wine** dataset. This is a 178 sample dataset that categorises 3 different types of Italian wine using 13 different features. The code below loads the Wine dataset and selects a subset of features for you to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d5631cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set matplotlib backend to inline\n",
    "%matplotlib inline\n",
    "\n",
    "# load data\n",
    "wine=datasets.load_wine()\n",
    "#print(wine.DESCR)\n",
    "\n",
    "# this dataset has 13 features, we will only choose a subset of these\n",
    "df_wine = pd.DataFrame(wine.data, columns = wine.feature_names )\n",
    "selected_features = ['alcohol','flavanoids','color_intensity','ash']\n",
    "\n",
    "# extract the data as numpy arrays of features, X, and target, y\n",
    "X = df_wine[selected_features].values\n",
    "y = wine.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3007bfa2",
   "metadata": {},
   "source": [
    "### 1.1. Visualising the data\n",
    "\n",
    "The first task is to create a grid for the **Wine** dataset, with each off-diagonal subplot showing the interaction between two features, and each of the classes represented as a different colour. The on-diagonal subplots (representing a single feature) should show a distribution (or histogram) for that feature.     \n",
    "\n",
    "Create a function that, given data X and labels y, plots this grid.  The function should be invoked something like this:\n",
    "        \n",
    "    myplotGrid(X,y,...)\n",
    "    \n",
    "where X is the training data and y are the labels. You can use an appropriate library to help you create the visualisation. You might want to code it yourself using matplotlib functions scatter and hist - however, this is not strictly necessary here, so try not spend too much time on this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4448b25b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_grid\u001b[39m(X, y, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hue_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      4\u001b[0m     X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minsert(X, \u001b[38;5;28mlen\u001b[39m(X[\u001b[38;5;241m0\u001b[39m]), y, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "def plot_grid(X, y, labels=None, hue_key=None):\n",
    "    X = np.insert(X, len(X[0]), y, 1)\n",
    "    labels = selected_features + [hue_key]\n",
    "    \n",
    "    data = pd.DataFrame(data=X, columns=labels)\n",
    "    g = sns.PairGrid(data, hue=hue_key)\n",
    "    g.map_diag(sns.kdeplot, fill=True)\n",
    "    g.map_offdiag(sns.scatterplot)\n",
    "    g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid(X, y, labels=selected_features, hue_key=\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab53cf",
   "metadata": {},
   "source": [
    "### 1.2. Exploratory Data Analysis under noise\n",
    "\n",
    "When data are collected under real-world settings they usually contain some amount of noise that makes classification more challenging. In the cell below, invoke your exploratory data analysis function above on a noisy version of your data X.\n",
    "\n",
    "Try to perturb your data with some Gaussian noise,\n",
    "\n",
    "    # initialize random seed to replicate results over different runs\n",
    "    mySeed = 12345 \n",
    "    np.random.seed(mySeed) \n",
    "    XN=X+np.random.normal(0,0.6,X.shape)\n",
    "    \n",
    "and then invoke\n",
    "\n",
    "    myplotGrid(XN,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturb data with noise to replicate real-world data\n",
    "# Use a seed to replicate results over different runs\n",
    "mySeed = 12 \n",
    "np.random.seed(mySeed) \n",
    "XN=X+np.random.normal(0,0.6,X.shape)\n",
    "plot_grid(XN, y, labels=selected_features, hue_key=\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340b6d8",
   "metadata": {},
   "source": [
    "## 2. Implementing kNN \n",
    "\n",
    "In the cell below, develop your own code for performing k-Nearest Neighbour classification. Define a function that performs k-NN given a set of data.  Your function should be invoked similary to:\n",
    "\n",
    "        y_ = mykNN(X,y,X_,options)\n",
    "        \n",
    "where X is your training data, y is your training outputs, X\\_ are your testing data and y\\_ are your predicted outputs for X\\_.  The options argument (can be a list or a set of separate arguments depending on how you choose to implement the function) should at least contain the number of neighbours to consider as well as the distance function employed.\n",
    "\n",
    "Hint: it helps to break the problem into various sub-problems, implemented as helper function. For example, you might want to implement separate function(s) for calculating the distances between two vectors. And another function that uncovers the nearest neighbour(s) to a given vector. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52521e27",
   "metadata": {},
   "source": [
    "### Helper functions for implementation of mykNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11509d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(data, split_ratio):\n",
    "    \n",
    "    \"\"\"Split a list-type into two by a split ratio\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : sliceable object\n",
    "    split_ratio : float (0 to 1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list that contains the elements from 0 to the split_ratio as its first element, and the rest as its second element\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(data)\n",
    "    \n",
    "    # Calculate cutoff index. Clamp split ratio up to 1. Clamp result between 0 and data length\n",
    "    cutoff_index = min(max(math.floor(total * min(split_ratio, 1)), 0), len(data))\n",
    "    \n",
    "    # Return sliced lists\n",
    "    return [np.array(data[0:cutoff_index]), np.array(data[cutoff_index:])]\n",
    "    \n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    \n",
    "    \"\"\"Computes Euclidean distance between two points\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p1 : array-like\n",
    "    p2 : array-like\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The Euclidean distance between both points\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 0.0\n",
    "    for i in range(len(p1)):\n",
    "        squared_diff = (p1[i] - p2[i])**2\n",
    "        distance += squared_diff\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "\n",
    "def manhattan_distance(p1, p2):\n",
    "    \n",
    "    \"\"\"Computes Manhattan distance between two points\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p1 : array-like\n",
    "    p2 : array-like\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The Manhattan distance between both points\n",
    "    \"\"\"\n",
    "    \n",
    "    zipped_points = zip(p1, p2)\n",
    "    differences = [abs(v1 - v2) for v1, v2 in zipped_points]\n",
    "    return sum(differences)\n",
    "\n",
    "\n",
    "# A p_value = 1 is equal to the Manhattan distance,\n",
    "# and p_value = 2 is equal to the Euclidean distance.\n",
    "# Minkowski distance is a generalization of both and\n",
    "# allows for more fine tuning of it by tweaking p_value.\n",
    "def minkowski_distance(p1, p2, p_value):\n",
    "    \n",
    "    \"\"\"Computes Minkowski distance between two points\n",
    "    \n",
    "    The Minkowski distance is a generalization of the\n",
    "    Manhattan and Euclidean distances that takes an extra\n",
    "    p_value parameter. A p_value = 1 is identical to the\n",
    "    Manhattan distance, whereas a p_value = 2 is identical\n",
    "    to the Euclidean distance. However, tweaking the values\n",
    "    in-betweeen allows for more fine grain control for\n",
    "    Machine Learning purposes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p1 : array-like\n",
    "    p2 : array-like\n",
    "    p_value : float, between 1 and 2\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The Minkowski distance between both points\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Clamp p_value to values between 1 and 2\n",
    "    p_value = min(max(p_value, 1), 2)\n",
    "    \n",
    "    # Zip our vector points for easier processing\n",
    "    zipped_points = zip(p1, p2)\n",
    "    \n",
    "    # Compute the squared difference of each component of our vector\n",
    "    squared_differences = [math.pow(abs(v1 - v2), p_value) for v1, v2 in zipped_points]\n",
    "    \n",
    "    # Sum the squared differences together\n",
    "    summed_differences = sum(squared_differences)\n",
    "    \n",
    "    # Compute our root p value\n",
    "    root_value = 1 / p_value\n",
    "    \n",
    "    # Return the Minkowski distance by raising to the power of our root value\n",
    "    return math.pow(summed_differences, root_value)\n",
    "\n",
    "\n",
    "def distance(p1, p2, dist_type=\"euclidean\"):\n",
    "    \n",
    "    \"\"\"Computes chosen distance type between two points\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p1 : array-like\n",
    "    p2 : array-like\n",
    "    dist_type : string or int or float\n",
    "        \"euclidean\" or \"manhattan\", or a number between 1 and 2\n",
    "        to return the minkowski distance using that as p value\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The distance between both points of the chosen type\n",
    "    \"\"\"\n",
    "    \n",
    "    if dist_type == \"manhattan\":\n",
    "        return manhattan_distance(p1, p2)\n",
    "    \n",
    "    elif dist_type == \"euclidean\":\n",
    "        return euclidean_distance(p1, p2)\n",
    "    \n",
    "    # If the dist_type is actually a number, then\n",
    "    # run the minkowski distance with dist_type as p\n",
    "    elif type(dist_type) == int or type(dist_type) == float:\n",
    "        return minkowski_distance(p1, p2, dist_type)\n",
    "    \n",
    "    # If dist_value doesn't fall on any of the above, default to euclidean\n",
    "    return euclidean_distance(p1, p2)\n",
    "\n",
    "    \n",
    "# Find the k nearest neighbours from a\n",
    "# series of points in space to a given point\n",
    "def k_nearest_neighbours(test_data_point, data_points, k, distance_type):\n",
    "    \n",
    "    \"\"\"Finds the k nearest neighbours among data_points to the test_data_point\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    test_data_point : list of values\n",
    "    data_points : two-dimensional array-like\n",
    "    k : int\n",
    "        Number of closest neighbours to return\n",
    "    distance_type : string\n",
    "        Either \"euclidean\" or \"manhattan\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of the closest data points to test_data_point among data_points\n",
    "    \"\"\"\n",
    "    \n",
    "    points = []\n",
    "    \n",
    "    # Clamp our number of neighbours to the length of our data\n",
    "    k = min(k, len(data_points))\n",
    "    \n",
    "    # Find the distances from all points to our testing point\n",
    "    for point in data_points:\n",
    "        dist = distance(point[\"data\"], test_data_point[\"data\"], distance_type)\n",
    "        point[\"distance\"] = dist\n",
    "        \n",
    "        # Append a pair of the point itself and the distance it's at\n",
    "        points.append(point)\n",
    "    \n",
    "    # Sort our pairs of point/distances by the distance they're at, ascendingly\n",
    "    points.sort(key = lambda point: point[\"distance\"])\n",
    "    \n",
    "    # Get the first k points of our distances point/distance pairs\n",
    "    neighbours = [points[i] for i in range(k)]\n",
    "    return neighbours\n",
    "\n",
    "\n",
    "def pick_class_from_neighbours(neighbours):\n",
    "    \n",
    "    \"\"\"Picks a class among a list of neighbours, either the most common, or breaking ties if any\n",
    "    \n",
    "    The tie breaking is made by averaging all distances to the test point for each class, then\n",
    "    picking the class with the lowest average distance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    neighbours : list of dicts\n",
    "    \n",
    "        This list is the same one that is returned by the k_nearest_neighbours method above.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    number\n",
    "        The selected label of the given test_point\n",
    "    \"\"\"\n",
    "    \n",
    "    classes = [ n[\"class\"] for n in neighbours ]\n",
    "    unique_classes = np.unique(classes)\n",
    "    max_class_occurrence = max(classes, key=classes.count)\n",
    "    max_classes = []\n",
    "\n",
    "    # Iterate through our unique classes, and add that\n",
    "    # class to max_classes if it's got the highest occurrence\n",
    "    for c in unique_classes:\n",
    "        if classes.count(c) == max_class_occurrence:\n",
    "            max_classes.append(c)\n",
    "\n",
    "    if len(max_classes) == 1:\n",
    "        # Classes aren't tied, so return the max one\n",
    "        return max_classes[0]\n",
    "\n",
    "    # Here, we've got a tie of neighbour class counts\n",
    "    # Calculate the average distance of each of them\n",
    "    lowest_avg_distance = math.inf\n",
    "    closest_avg_class = None\n",
    "\n",
    "    # Find the avg distance among the members of a class.\n",
    "    # If it is lower than the lowest recorded so far, record it.\n",
    "    for c in max_classes:\n",
    "        class_neighbours = [ n for n in neighbours if n[\"class\"] == c ]\n",
    "        distances = [ n[\"distance\"] for n in class_neighbours ]\n",
    "        avg_distance = np.mean(distances)\n",
    "\n",
    "        if avg_distance < lowest_avg_distance:\n",
    "            lowest_avg_distance = avg_distance\n",
    "            closest_avg_class = c\n",
    "\n",
    "    # Finally, return the class with the lowest avg distance\n",
    "    return c\n",
    "\n",
    "\n",
    "\n",
    "def predict_class(test_point, train_data, k, distance_type, break_ties=False):\n",
    "    \n",
    "    \"\"\"Predicts the label of test_point by using the elements in train_data and k-nearest-neighbours calculated with chosen distance_type\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    test_point : list of values\n",
    "    train_data : two-dimensional array-like\n",
    "    k : int\n",
    "        Number of closest neighbours to return\n",
    "    distance_type : string\n",
    "        Either \"euclidean\" or \"manhattan\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    number\n",
    "        The predicted label of the given test_point\n",
    "    \"\"\"\n",
    "    \n",
    "    neighbours = k_nearest_neighbours(test_point, train_data, k, distance_type)\n",
    "    \n",
    "    # If the more advanced method to break ties is enabled, use the above defined function\n",
    "    if break_ties == True and len(neighbours) > 1:\n",
    "        return pick_class_from_neighbours(neighbours)\n",
    "    \n",
    "    # Otherwise just return the most common class across neighbours\n",
    "    classes = [item[\"class\"] for item in neighbours]\n",
    "    prediction = max(classes, key=classes.count)\n",
    "    return prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c750484",
   "metadata": {},
   "source": [
    "### Implementation of mykNN using all of the above helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda33f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# mykNN code\n",
    "def mykNN(X, y, X_, k=5, distance_type=\"euclidean\", break_ties=False):\n",
    "    \n",
    "    \"\"\"Predicts the labels of a split of the data in X\n",
    "    \n",
    "    k-Nearest-Neighbour algorithm to predict the labels of the provided split within the dataset X, \n",
    "    using the complementary split and the target labels y as training data. Algorithm is set to use\n",
    "    a number of nearest neighbours for its predictions equal to k, and a distance algorithm of the\n",
    "    type chosen in distance_type.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : two-dimensional array-like\n",
    "        The full dataset without labels\n",
    "        \n",
    "    y : list\n",
    "        The list of labels with indexes matching the rows in X\n",
    "        \n",
    "    k : int\n",
    "        Number of closest neighbours to calculate in the prediction\n",
    "        \n",
    "    distance_type : string\n",
    "        Either \"euclidean\" or \"manhattan\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        The list of predicted labels for the split test data\n",
    "    \"\"\"\n",
    "    \n",
    "    classified_train_data = [{ \"data\": X[i], \"class\": y[i] } for i in range(0, len(X))]\n",
    "    test_data = [{ \"data\": X_[i] } for i in range(0, len(X_))]\n",
    "    predicted_classes = []\n",
    "    \n",
    "    for test_item in test_data:\n",
    "        predicted_class = predict_class(test_item, classified_train_data, k, distance_type, break_ties)\n",
    "        predicted_classes.append(predicted_class)\n",
    "        \n",
    "    return np.array(predicted_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6c93e",
   "metadata": {},
   "source": [
    "### Test run of mykNN() to ensure it's working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ceba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random seed to create a permutation of our dataset\n",
    "np.random.seed(seed=420)\n",
    "perm_X = np.random.permutation(X)\n",
    "np.random.seed(seed=420)\n",
    "perm_y = np.random.permutation(y)\n",
    "\n",
    "# Split the dataset into 80% training, 20% testing\n",
    "train_data, test_data = split_data(perm_X, split_ratio=0.8)\n",
    "train_classes, test_classes = split_data(perm_y, split_ratio=0.8)\n",
    "\n",
    "# Run mykNN and get the predicted classes\n",
    "predicted_classes = mykNN(train_data, train_labels, test_data, k=10, distance_type=\"manhattan\")\n",
    "print(\"Predicted classes:\\t\" + str(predicted_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb9dc65",
   "metadata": {},
   "source": [
    "## 3. Classifier evaluation \n",
    "\n",
    "In the cell below, implement your own classifier evaluation code. This should include some way of calculating confusion matrices, as well as common metrics like accuracy. \n",
    "\n",
    "Write some additional code that lets you display the output of your confusion matrices in a useful and easy-to-read manner.\n",
    "\n",
    "You might want to test your functions on some test data, and compare the results to the sklearn library versions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f81893",
   "metadata": {},
   "source": [
    "### Performance metrics functions to evaluate our classifier results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a644af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_confusion_matrix(true_y, predicted_y):\n",
    "    \n",
    "    \"\"\"Returns the confusion matrix of ground truths vs. predicted labels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_y : list\n",
    "        The ground truths, or true labels\n",
    "        \n",
    "    predicted_y : list\n",
    "        The labels predicted by the classifier\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A two-dimensional numpy array; the confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    nbr_of_classes = len(np.unique(true_y))\n",
    "    \n",
    "    # Create an empty 2d array of zeroes, with as many rows and columns as classes\n",
    "    matrix = np.zeros((nbr_of_classes, nbr_of_classes), dtype=int)\n",
    "    \n",
    "    # Iterate through each result of the classifier\n",
    "    for i in range(0, len(predicted_y)):\n",
    "        # Access the matrix cell indicated by the true class number\n",
    "        # and the predicted one, and add one result there\n",
    "        matrix[true_y[i], predicted_y[i]] += 1\n",
    "            \n",
    "    return matrix\n",
    "\n",
    "\n",
    "def classifier_accuracy(true_y, predicted_y):\n",
    "    \n",
    "    \"\"\"Returns the accuracy of the results of a classifier\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_y : list\n",
    "        The ground truths, or true labels\n",
    "        \n",
    "    predicted_y : list\n",
    "        The labels predicted by the classifier\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The accuracy ratio or percentage (0 to 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the number of correct predictions\n",
    "    predicted = np.where(true_y == predicted_y, 1, 0)\n",
    "    \n",
    "    # Get the total number of elements\n",
    "    total = len(true_y)\n",
    "    \n",
    "    # Return the correct predictions divided by the number of elements\n",
    "    return sum(predicted) / total\n",
    "\n",
    "\n",
    "def classifier_class_recall(confusion_matrix):\n",
    "    \n",
    "    \"\"\"Returns the per-class recall rate of a confusion matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    confusion_matrix : np.ndarray\n",
    "        A two-dimensional, square array\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        The list of recall rate per class (0 to 1 for each element)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a zero array with as many elements as classes\n",
    "    recall = np.zeros(len(confusion_matrix))\n",
    "    \n",
    "    # Iterate through each class\n",
    "    for i in range(0, len(confusion_matrix)):\n",
    "        # Get the diagonal (correct number of predicted values) of that class\n",
    "        diagonal_value = confusion_matrix[i][i]\n",
    "        \n",
    "        # Get the number of items of that class in this data\n",
    "        row_sum = sum(confusion_matrix[i])\n",
    "        \n",
    "        # Calculate the percentage of times it predicted that class correctly\n",
    "        recall[i] = diagonal_value / row_sum\n",
    "        \n",
    "    return recall\n",
    "\n",
    "\n",
    "def classifier_precision(confusion_matrix):\n",
    "    \n",
    "    \"\"\"Returns the per-class precision rate of a confusion matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    confusion_matrix : np.ndarray\n",
    "        A two-dimensional, square array\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        The list of precision rate per class (0 to 1 for each element)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a zero array with as many elements as classes\n",
    "    precision = np.zeros(len(confusion_matrix))\n",
    "    \n",
    "    # Iterate through each class\n",
    "    for i in range(0, len(confusion_matrix)):\n",
    "        # Get the diagonal (correct number of predicted values) of that class\n",
    "        diagonal_value = confusion_matrix[i][i]\n",
    "        \n",
    "        # Get the prediction column values for that class in one list\n",
    "        column = [ confusion_matrix[row][i] for row in range(0, len(confusion_matrix)) ]\n",
    "        \n",
    "        # Get the number of times the classifier predicted that class (wrong or not)\n",
    "        column_sum = sum(column)\n",
    "        \n",
    "        # Calculate the percentage of times it predicted that class correctly\n",
    "        precision[i] = diagonal_value / column_sum\n",
    "        \n",
    "    return precision\n",
    "\n",
    "\n",
    "def right_pad_str(str_to_pad, padding, char=\" \"):\n",
    "    \n",
    "    \"\"\"Adds padding to the right of a string\n",
    "    \n",
    "    Accounts for the str length to create even columns that result in the same width\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    str_to_pad : stringifiable value\n",
    "        The value to add padding to\n",
    "        \n",
    "    padding : int\n",
    "        The amount of padding characters to add\n",
    "        \n",
    "    char : string\n",
    "        The character to use as padding\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "    \"\"\"\n",
    "    \n",
    "    return str(str_to_pad) + char * max(0, padding - len(str(str_to_pad)))\n",
    "\n",
    "\n",
    "def center_pad_str(str_to_pad, padding, char=\" \"):\n",
    "    \n",
    "    \"\"\"Adds equal padding on both sides of a string\n",
    "    \n",
    "    Accounts for the str length to create even columns that result in the same width\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    str_to_pad : stringifiable value\n",
    "        The value to add padding to\n",
    "        \n",
    "    padding : int\n",
    "        The amount of padding characters to add\n",
    "        \n",
    "    char : string\n",
    "        The character to use as padding\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "    \"\"\"\n",
    "    stringified = str(str_to_pad)\n",
    "    padding = max(0, padding - len(stringified))\n",
    "    \n",
    "    if padding <= 0:\n",
    "        return stringified\n",
    "                  \n",
    "    return char * math.floor(padding*0.5) + stringified + char * math.ceil(padding*0.5)\n",
    "\n",
    "\n",
    "\n",
    "def format_confusion_matrix(matrix, ground_truths_label=\"GROUND TRUTHS\", predictions_label=\"PREDICTIONS\", add_recall=False, per_class_recall=None):\n",
    "    \n",
    "    \"\"\"Formats a confusion matrix into a printable, readable string\n",
    "    \n",
    "    If add_recall is True, it will also add the per-class recall metrics\n",
    "    at the end of each class row.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : 2-dimensional array-like\n",
    "        The confusion matrix to format\n",
    "        \n",
    "    ground_truths_label : string\n",
    "        The label that will be displayed to the left of the table, indicating rows are the ground truths\n",
    "        \n",
    "    predictions_label : string\n",
    "        The label that will be displayed above the table columns, indicating they are the predictions\n",
    "        \n",
    "    add_recall : boolean\n",
    "        Whether or not to add the per-class recall metrics to the right side of each row\n",
    "        \n",
    "    per_class_recall : list\n",
    "        The list of recall rate per class (0 to 1 for each element). If add_recall is True this must\n",
    "        be passed in as well to be able to format the class recall into the table\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Some helpful variables for building up our table string\n",
    "    class_label = \"CLASS \"\n",
    "    class_label_len = len(class_label) + 2\n",
    "    col_separator = \" | \"\n",
    "    \n",
    "    # The empty space to the left of the first column of the table, taking into account the length\n",
    "    # of the given ground truths label and the class labels that will be to the left of the table rows\n",
    "    ground_truths_label_padding = \" \" * len(ground_truths_label)\n",
    "    class_row_label_padding = \" \" * class_label_len\n",
    "    left_empty_table_space = ground_truths_label_padding + (\" \" * len(col_separator)) + class_row_label_padding\n",
    "    \n",
    "    # Helper variables for padding in each row of content of the string\n",
    "    row_space_taken_by_col_separators = len(col_separator) * (len(matrix) + 1)\n",
    "    row_space_taken_by_class_labels = len(matrix) * class_label_len\n",
    "    \n",
    "    # A table row horizontal division line\n",
    "    row_division_line = \" \" + \"-\" * (row_space_taken_by_col_separators + row_space_taken_by_class_labels - 2) + \"\\n\"\n",
    "    \n",
    "    # The predictions_label that will be displayed above the columns and the table\n",
    "    predictions_label_row = left_empty_table_space + center_pad_str(predictions_label, row_space_taken_by_col_separators + row_space_taken_by_class_labels) + \"\\n\"\n",
    "    \n",
    "    # The divisive horizontal line below the predictions_label\n",
    "    predictions_label_bottom_divisive_line = left_empty_table_space + row_division_line\n",
    "    \n",
    "    # Create a header string for each of the column class headers\n",
    "    col_class_labels = [ \n",
    "        center_pad_str(class_label + str(i), class_label_len) \n",
    "        for i \n",
    "        in range(0, len(matrix)) \n",
    "    ]\n",
    "    \n",
    "    # Create the full header of each column of the table, showing the class names, like \"CLASS 0\", \"CLASS 1\", etc.\n",
    "    col_headers = left_empty_table_space + col_separator + col_separator.join(col_class_labels) + col_separator + \"\\n\"\n",
    "    \n",
    "    # Our final table string so far\n",
    "    table_header = predictions_label_row + predictions_label_bottom_divisive_line + col_headers + predictions_label_bottom_divisive_line\n",
    "    \n",
    "    # The rows of content themselves that we will build through a loop\n",
    "    table_rows = \"\"\n",
    "    \n",
    "    # Iterate through our rows\n",
    "    for row in range(0, len(matrix)):\n",
    "        \n",
    "        # If we're at roughly half of our rows, add the \"GROUND TRUTHS\" label to the left of the class name, and then the current class name\n",
    "        if (row == math.floor(len(matrix)/2)):\n",
    "            table_rows += ground_truths_label + col_separator + class_label + str(row) + \"\\t\"\n",
    "        \n",
    "        # Otherwise just indent enough space to account for the \"GROUND TRUTHS\" label and write the class name\n",
    "        else:\n",
    "            table_rows += ground_truths_label_padding + col_separator + class_label + str(row) + \"\\t\"\n",
    "        \n",
    "        # Iterate through our columns\n",
    "        for col in range(0, len(matrix[row])):\n",
    "            \n",
    "            # Get the value of this particular cell\n",
    "            cell_val = matrix[row][col]\n",
    "            cell_str = center_pad_str(cell_val, class_label_len)\n",
    "            \n",
    "            # Create the cell, with one | to its left, and place the value in the half space of the length of the class labels (so that it's centered in the cell)\n",
    "            table_rows += col_separator + cell_str\n",
    "            \n",
    "        # Close out our last cell of the row with the last | character\n",
    "        table_rows += col_separator\n",
    "        \n",
    "        # If the option is True, write out the class recall value for this row\n",
    "        if add_recall == True and len(per_class_recall) == len(matrix):\n",
    "            table_rows += \"\\tRecall: %.2f\" % (per_class_recall[row] * 100) + \"%\"\n",
    "        \n",
    "        # Break line to create a horizontal separation line to close out our cells,\n",
    "        # and add the blank space to account for the \"GROUND TRUTHS\" label on the left\n",
    "        table_rows += \"\\n\" + ground_truths_label_padding\n",
    "        \n",
    "        # If we're not at the very last row, add a | character to continue the vertical line\n",
    "        # that separates the \"GROUND TRUTHS\" label and the class row labels\n",
    "        if (row < len(matrix) - 1):\n",
    "            table_rows += col_separator\n",
    "            \n",
    "        # If it's the last row we don't need the | character, as it looks bad, so pad with spaces\n",
    "        else:\n",
    "            table_rows += \" \" * len(col_separator)\n",
    "            \n",
    "        # Create the row's bottom horizontal division line\n",
    "        table_rows += class_row_label_padding + row_division_line\n",
    "        \n",
    "    return table_header + table_rows\n",
    "    \n",
    "\n",
    "# Calculates and displays performance metrics based\n",
    "# on the ground truths and classifier predictions\n",
    "def display_metrics(true_y, predicted_y):\n",
    "    \n",
    "    \"\"\"Calculates and prints several performance metrics\n",
    "    \n",
    "    Specifically, using the list of known labels or ground truths, and the list\n",
    "    of predicted labels, this function will calculate and pretty print:\n",
    "    \n",
    "    Accuracy\n",
    "    Confusion Matrix\n",
    "    Per-class recall\n",
    "    Per-class precision\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_y : list\n",
    "        The ground truths, or true labels\n",
    "        \n",
    "    predicted_y : list\n",
    "        The labels predicted by the classifier\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the overall accuracy\n",
    "    accuracy = classifier_accuracy(true_y, predicted_y)\n",
    "    \n",
    "    # Get the confusion matrix\n",
    "    matrix = create_confusion_matrix(true_y, predicted_y)\n",
    "    \n",
    "    # Get the recall per class (list)\n",
    "    class_recall = classifier_class_recall(matrix)\n",
    "    \n",
    "    # Get the precision per class (list)\n",
    "    precision = classifier_precision(matrix)\n",
    "    \n",
    "    # Some helpful variables for building up our table string\n",
    "    ground_truths_label = \"GROUND TRUTHS\"\n",
    "    predictions_label = \"PREDICTIONS\"\n",
    "    \n",
    "    # Header of our display, showing the overall accuracy\n",
    "    header = \"CLASSIFIER PERFORMANCE\\n\" + \"-\" * ((len(matrix) * 10)+1) + \"\\n\\tAccuracy: %.2f\" % (accuracy * 100) + \"%\\n\\n\"\n",
    "    \n",
    "    formatted_matrix = format_confusion_matrix(matrix, ground_truths_label, predictions_label, add_recall=True, per_class_recall=class_recall)\n",
    "    \n",
    "    # Footer of our display, showing the class-relative precision\n",
    "    precision_footer = \"\\n\\nCLASS PRECISION:\\n\" + \"-\" * ((len(matrix) * 10)+1) + \"\\n\" + \"\".join([\"\\tClass \" + str(i) + \": %.2f\" % (precision[i] * 100) + \"%\\n\" for i in range(0, len(matrix))])\n",
    "        \n",
    "    # Print our output\n",
    "    print(header + formatted_matrix + precision_footer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a0a55b",
   "metadata": {},
   "source": [
    "### Classifier Performance metrics with the test results obtained from the test run of mykNN():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation code\n",
    "display_metrics(test_classes, predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69fa988",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Nested Cross-validation using your implementation of KNN  \n",
    "\n",
    "In the cell below, develop your own code for performing 5-fold nested cross-validation along with your implemenation of k-NN above. You must write your own code -- the scikit-learn module may only be used for verification purposes. \n",
    "\n",
    "Your code for nested cross-validation should invoke your kNN function (see above). You cross validation function should be invoked similary to:\n",
    "\n",
    "    accuracies_fold = myNestedCrossVal(X,y,5,list(range(1,11)),['euclidean','manhattan'],mySeed)\n",
    "    \n",
    "where X is your data matrix (containing all samples and features for each sample), 5 is the number of folds, y are your known output labels, ``list(range(1,11)`` evaluates the neighbour parameter from 1 to 10, and ``['euclidean','manhattan',...]`` evaluates the distances on the validation sets.  mySeed is simply a random seed to enable us to replicate your results.\n",
    "\n",
    "**Notes:** \n",
    "- you should perform nested cross-validation on **both** your original data X, as well as the data pertrubed by noise as shown in the cells above (XN)\n",
    "- you should evaluate **at least** two distance functions\n",
    "- you should evaluate number of neighbours from 1 to 10\n",
    "- your function should return a list of accuracies per fold\n",
    "- for each **fold**, your function should print:\n",
    "  - the accuracy per distinct set of parameters on the validation set\n",
    "  - the best set of parameters for the fold after validation\n",
    "  - the confusion matrix per fold (on the testing set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0a095",
   "metadata": {},
   "source": [
    "### Nested Cross Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def splice_data(data, start, end):\n",
    "    \n",
    "    \"\"\"Removes elements from start to end in a list, and returns the list without them\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list-like\n",
    "        A sliceable list or array\n",
    "        \n",
    "    start : int\n",
    "        The start index to be removed\n",
    "        \n",
    "    end : int\n",
    "        The range from start to end (not included)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list-like\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the start and end indexes are out of bounds,\n",
    "    # return the given data as it was passed\n",
    "    if start <= 0 and end >= len(data):\n",
    "        return data\n",
    "    \n",
    "    # Get the slice from 0 to start\n",
    "    start_slice = data[0:start]\n",
    "    \n",
    "    # Get the slice from end to the list length\n",
    "    end_slice = data[end:]\n",
    "    \n",
    "    # If the start slice has no elements because start is 0,\n",
    "    # return only the end slice\n",
    "    if start <= 0:\n",
    "        return np.array(end_slice)\n",
    "    \n",
    "    # If the end slice has no elements because end\n",
    "    # is out of bounds, return only the start slice\n",
    "    elif end >= len(data):\n",
    "        return np.array(start_slice)\n",
    "    \n",
    "    # Put together our two slices and return them\n",
    "    return np.concatenate((start_slice, end_slice))\n",
    "\n",
    "\n",
    "def n_fold_split(X, y, fold_nbr):\n",
    "    \n",
    "    \"\"\"Create n training and test folds of a dataset\n",
    "    \n",
    "    This respects the separation between training and\n",
    "    test data, creating the split between both at\n",
    "    separate intervals relative to the number of folds\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d array\n",
    "        The dataset to be folded into n slices\n",
    "        \n",
    "    y : list\n",
    "        The classes of each element of the dataset,\n",
    "        ordered the same way as X\n",
    "        \n",
    "    fold_nbr : int\n",
    "        The number of folds or slices to create\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list \n",
    "        Where [0] is training folds, [1] is training label folds,\n",
    "        [2] is test folds, [3] is test label folds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clamp our fold number to 1 or more\n",
    "    fold_nbr = max(fold_nbr, 1)\n",
    "    \n",
    "    # Find out the size of each fold\n",
    "    fold_size = min(math.ceil(len(X) / fold_nbr), len(X))\n",
    "    \n",
    "    # Create empty lists to store each of the folds\n",
    "    train_slices = list()\n",
    "    train_label_slices = list()\n",
    "    \n",
    "    test_slices = list()\n",
    "    test_label_slices = list()\n",
    "    \n",
    "    # For each fold...\n",
    "    for i in range(0, fold_nbr):\n",
    "        \n",
    "        # Get our start index for the test data, shifting it by fold\n",
    "        test_start = i*fold_size\n",
    "        \n",
    "        # Get our end index for the test data, shifting it by fold\n",
    "        test_end = (i+1)*fold_size\n",
    "        \n",
    "        # Get a test slice and labels with the given indexes\n",
    "        test_slices.append( X[test_start : test_end] )\n",
    "        test_label_slices.append( y[test_start : test_end] )\n",
    "        \n",
    "        # Get the training and label slices by splicing away the test slices\n",
    "        train_slices.append( splice_data(X, test_start, test_end) )\n",
    "        train_label_slices.append( splice_data(y, test_start, test_end) )\n",
    "\n",
    "    # Return our folds, so that result[0] is training folds, result[1] is training label folds,\n",
    "    # result[2] is test folds, and result[3] is test label folds\n",
    "    return [ train_slices, train_label_slices, test_slices, test_label_slices ]\n",
    "\n",
    "\n",
    "def create_param_permutations(*args):\n",
    "    \n",
    "    \"\"\"Create all possible permutations in a list of lists\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    args : list\n",
    "        A series of list-like parameters to permutate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of lists \n",
    "        Each list within the list is a given permutation of\n",
    "        the parameters, in the same order in which the param\n",
    "        lists were passed to the function\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(itertools.product(*args))\n",
    "    \n",
    "\n",
    "def get_best_params(X, y, k_neighbours_list, distance_types, break_ties=False):\n",
    "    \n",
    "    \"\"\"Finds out the accuracy in a dataset after testing all possible param permutations\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d array\n",
    "        The dataset for which to find the best parameters\n",
    "        \n",
    "    y : list\n",
    "        The class labels corresponding to the dataset\n",
    "        \n",
    "    k_neighbours_list : list\n",
    "        The range of k_neighbours parameters to test\n",
    "        \n",
    "    distance_types : list\n",
    "        The range of distance type parameters to test\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict \n",
    "        A dict object with the following keys:\n",
    "            \"accuracies\": the list of accuracies of each permutation\n",
    "            \"highest_accuracy\": the highest accuracy achieved\n",
    "            \"best_params\": the best param permutation that achieved the highest accuracy\n",
    "            \"best_matrix\": the confusion matrix of the best result obtained\n",
    "            \"best_preciction\": the list of predicted classes for the validation set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split our data into training and validation lists\n",
    "    train_data, validation_data = split_data(X, split_ratio=0.8)\n",
    "    train_labels, validation_labels = split_data(y, split_ratio=0.8)\n",
    "    \n",
    "    # Get all possible param permutations. Each element is a list\n",
    "    # of params in the same order that they were passed to the function\n",
    "    param_permutations = create_param_permutations(k_neighbours_list, distance_types)\n",
    "    \n",
    "    # Initialize variables to store our final results\n",
    "    accuracies = list()\n",
    "    best_permutation = list()\n",
    "    best_prediction = list()\n",
    "    highest_accuracy = 0\n",
    "    matrix_of_best_result = None\n",
    "    \n",
    "    # For each possible param permutation...\n",
    "    for permutation in param_permutations:\n",
    "        \n",
    "        # Test the set of params\n",
    "        predicted_labels = mykNN(train_data, train_labels, validation_data, permutation[0], permutation[1], break_ties)\n",
    "        \n",
    "        # Calculate the accuracy with this set of params\n",
    "        accuracy = classifier_accuracy(validation_labels, predicted_labels)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        print(\"k=\" + str(permutation[0]) + \"\\tdist=\" + right_pad_str(\"'\" + str(permutation[1]) + \"'\", 15) + \" %.2f\" % accuracy + \" Accuracy\")\n",
    "        \n",
    "        # If the accuracy is higher than ever before, store it as highest accuracy, along with the param permutation and prediction\n",
    "        if accuracy > highest_accuracy:\n",
    "            best_prediction = predicted_labels\n",
    "            highest_accuracy = accuracy\n",
    "            best_permutation = permutation\n",
    "            \n",
    "    print(\"\\n-->Highest accuracy %.2f\" % highest_accuracy + \" validated with k=\" + str(best_permutation[0]) + \" and distance_type='\" + str(best_permutation[1]) + \"'.\")\n",
    "    \n",
    "    # Return all of our results\n",
    "    return { \n",
    "        \"accuracies\": accuracies,\n",
    "        \"highest_accuracy\": highest_accuracy,\n",
    "        \"best_params\": best_permutation,\n",
    "        \"best_matrix\": create_confusion_matrix(train_labels, best_prediction),\n",
    "        \"best_prediction\": best_prediction\n",
    "    }\n",
    "\n",
    "\n",
    "# The Minkowski distance calculation was added to the range of suggested parameters, including several of its possible p values: 1.25, 1.5 and 1.75\n",
    "def myNestedCrossVal(X, y, fold_nbr=5, k_neighbours_list=list(range(1, 11)), distance_types=[ \"euclidean\", \"manhattan\", 1.25, 1.5, 1.75 ], seed=420):\n",
    "    \n",
    "    \"\"\"Performs Nested Cross Validation with n folds on a given dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d array\n",
    "        The dataset on which to perform the algorithm\n",
    "        \n",
    "    y : list\n",
    "        The class labels corresponding to the dataset\n",
    "        \n",
    "    fold_nbr : int\n",
    "        The number of folds to create for which to find the best params\n",
    "        \n",
    "    k_neighbours_list : list\n",
    "        The range of k_neighbours parameters to test\n",
    "        \n",
    "    distance_types : list\n",
    "        The range of distance type parameters to test\n",
    "        \n",
    "    seed : int\n",
    "        The seed to use to shuffle the dataset elements around before the nested cross validation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts\n",
    "        A list of dict objects, one for each fold, with the following keys:\n",
    "            \"accuracy\": the accuracy achieved on this fold's test data\n",
    "            \"matrix\": the confusion matrix for this fold's test data\n",
    "            \"truths\": the class labels for this fold's test data\n",
    "            \"predictions\": the class prediction results for this fold's test data\n",
    "            \"parameters\": the best params that were used for this fold's test data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Shuffle our dataset elements around using the given seed\n",
    "    np.random.seed(seed=seed)\n",
    "    perm_X = np.random.permutation(X)\n",
    "    np.random.seed(seed=seed)\n",
    "    perm_y = np.random.permutation(y)\n",
    "          \n",
    "    # Using the shuffled dataset, create n folds, each with their independent slice of data for training and testing\n",
    "    fold_train_slices, fold_train_label_slices, fold_test_slices, fold_test_label_slices = n_fold_split(perm_X, perm_y, fold_nbr)\n",
    "    fold_results = list()\n",
    "          \n",
    "    # Iterate through each of our folds...\n",
    "    for i in range(0, len(fold_train_slices)):\n",
    "        print(\"\\n\\nFOLD \" + str(i+1) + \"\\n\" + \"#\" * 75)\n",
    "        print(\"\\nTraining for best parameters...\\n\")\n",
    "        \n",
    "        # Get the best params for this fold's training data\n",
    "        validation_result = get_best_params(fold_train_slices[i], fold_train_label_slices[i], k_neighbours_list, distance_types, break_ties)\n",
    "        params = validation_result[\"best_params\"]\n",
    "        \n",
    "        print(\"\\nApplying best parameters to test data...\\n\")\n",
    "        \n",
    "        # Run the k-nearest-neighbour algorithm on this fold's test data, using the best params that the training data found\n",
    "        fold_test_predictions = mykNN(fold_train_slices[i], fold_train_label_slices[i], fold_test_slices[i], params[0], params[1])\n",
    "        \n",
    "        # Calculate accuracy of this fold's test data\n",
    "        accuracy = classifier_accuracy(fold_test_label_slices[i], fold_test_predictions)\n",
    "        print(\"-->Fold \" + str(i+1) + \"'s Test data's Accuracy with best parameters k=\" + str(params[0]) + \" and distance_type='\" + str(params[1]) + \"': %.2f\" % accuracy + \"\\n\")\n",
    "        # Create matrix of this fold's test data\n",
    "        matrix = create_confusion_matrix(fold_test_label_slices[i], fold_test_predictions)\n",
    "        print(\"-->Fold \" + str(i+1) + \"'s Test data's Confusion Matrix with best parameters:\\n\")\n",
    "        print(format_confusion_matrix(matrix))\n",
    "        \n",
    "        # Create our results dict object for this fold and append it to our final list\n",
    "        fold_results.append({\n",
    "            \"accuracy\": accuracy,\n",
    "            \"matrix\": matrix,\n",
    "            \"truths\": fold_test_label_slices[i],\n",
    "            \"predictions\": fold_test_predictions,\n",
    "            \"parameters\": params\n",
    "        })\n",
    "        \n",
    "        #display_metrics(fold_test_label_slices[i], fold_test_predictions)\n",
    "        print(\"#\" * 75)\n",
    "        \n",
    "    #print(\"AVERAGE ACCURACY OF FOLDS IS %.2f\" % (np.mean(final_accuracies) * 100) + \"%!\")\n",
    "    return fold_results\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46cb17",
   "metadata": {},
   "source": [
    "### Clean Nested Cross Validation Results (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f91577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate clean data code\n",
    "clean_results = myNestedCrossVal(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc71fda",
   "metadata": {},
   "source": [
    "### Noisy Nested Cross Validation Results (XN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate noisy data code\n",
    "noisy_results = myNestedCrossVal(XN, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf8319",
   "metadata": {},
   "source": [
    "### Summary of the results of the clean dataset (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add787cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summaries\n",
    "clean_accuracies = [ r[\"accuracy\"] for r in clean_results ]\n",
    "clean_average_accuracy = np.mean(clean_accuracies)\n",
    "clean_std = np.std(clean_accuracies)\n",
    "\n",
    "print(\"_\" * 75 + \"\\nSUMMARY OF CLEAN RESULTS\")\n",
    "print(\"AVERAGE ACCURACY:\\t %.2f\" % clean_average_accuracy)\n",
    "print(\"STD:\\t\\t\\t %.2f\" % clean_std + \"\\n\" + \"_\" * 75)\n",
    "\n",
    "for i in range(0, len(clean_results)):\n",
    "    print(\"\\nCLEAN FOLD \" + str(i+1) + \" SUMMARY:\")\n",
    "    print(\"\\nPARAMETERS:\\tk_neighbours: \" + str(clean_results[i][\"parameters\"][0]) + \", distance type: '\" + str(clean_results[i][\"parameters\"][1]) + \"'\\n\")\n",
    "    display_metrics(clean_results[i][\"truths\"], clean_results[i][\"predictions\"])\n",
    "    print(\"#\" * 75 + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1977dc9",
   "metadata": {},
   "source": [
    "### Summary of the results of the noisy dataset (XN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_accuracies = [ r[\"accuracy\"] for r in noisy_results ]\n",
    "noisy_average_accuracy = np.mean(noisy_accuracies)\n",
    "noisy_std = np.std(noisy_accuracies)\n",
    "\n",
    "print(\"_\" * 75 + \"\\nSUMMARY OF NOISY RESULTS\")\n",
    "print(\"AVERAGE ACCURACY:\\t %.2f\" % noisy_average_accuracy)\n",
    "print(\"STD:\\t\\t\\t %.2f\" % noisy_std + \"\\n\" + \"_\" * 75)\n",
    "\n",
    "for i in range(0, len(noisy_results)):\n",
    "    print(\"\\nNOISY FOLD \" + str(i+1) + \" SUMMARY:\\n\")\n",
    "    print(\"PARAMETERS:\\tk_neighbours: \" + str(noisy_results[i][\"parameters\"][0]) + \", distance type: '\" + str(noisy_results[i][\"parameters\"][1]) + \"'\\n\")\n",
    "    display_metrics(noisy_results[i][\"truths\"], noisy_results[i][\"predictions\"])\n",
    "    print(\"#\" * 75 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa3d4de",
   "metadata": {},
   "source": [
    "## 5. Summary of results\n",
    "\n",
    "Using your results from above, fill out the following table using the **clean** data:\n",
    "\n",
    "| Fold | accuracy |  k  | distance |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | .8889  |  1 | euclidean  |\n",
    "| 2 | .9444  |  1 | euclidean  |\n",
    "| 3 | .9722  |  1 | manhattan  |\n",
    "| 4 | .9167  |  1 | euclidean  |\n",
    "| 5 | .9118  |  3 | euclidean  |\n",
    "| **total** | .93 $\\pm$ 0.03 |   |    |\n",
    "\n",
    "Where **total** is given as an average over all the folds, and $\\pm$ the standard deviation.\n",
    "\n",
    "Now fill out the following table using the **noisy** data:\n",
    "\n",
    "| Fold | accuracy |  k  | distance |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | .7778  |  3 | manhattan  |\n",
    "| 2 | .8333  |  5 | minkowski (1.25)  |\n",
    "| 3 | .8611  |  6 | minkowski (1.25)  |\n",
    "| 4 | .8889  |  4 | manhattan  |\n",
    "| 5 | .8235  |  3 | euclidean  |\n",
    "| **total** | .84 $\\pm$ 0.04 |   |    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37204d",
   "metadata": {},
   "source": [
    "### 5.2. Confusion matrix summary\n",
    "\n",
    "Summarise the overall results of your nested cross validation evaluation of your K-NN algorithm using two summary confusion matrices (one for the noisy data, one for the clean data). You might want to adapt your ```myNestedCrossVal``` code above to also return a list of confusion matrices.\n",
    "\n",
    "Use or adapt your evaluation code above to print the two confusion matrices below. Make sure you label the matrix rows and columns. You might also want ot show class-relative precision and recall. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566819f0",
   "metadata": {},
   "source": [
    "#### Summary of Clean Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a361530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CLEAN RESULTS SUMMARY\\n\" + \"_\" * 75 + \"\\n\\n\")\n",
    "clean_truths = np.concatenate([ r[\"truths\"] for r in clean_results ])\n",
    "clean_predictions = np.concatenate([ r[\"predictions\"] for r in clean_results ])\n",
    "display_metrics(clean_truths, clean_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb183a",
   "metadata": {},
   "source": [
    "#### Summary of Noisy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a235f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNOISY RESULTS SUMMARY\\n\" + \"_\" * 75 + \"\\n\\n\")\n",
    "noisy_truths = np.concatenate([ r[\"truths\"] for r in noisy_results ])\n",
    "noisy_predictions = np.concatenate([ r[\"predictions\"] for r in noisy_results ])\n",
    "display_metrics(noisy_truths, noisy_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
